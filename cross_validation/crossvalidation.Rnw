\documentclass[a4paper]{article}

\usepackage{Sweave} %--------------------------------!
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage[usenames, dvipsnames]{color}
\usepackage{verbatim}

\oddsidemargin 0cm
\topmargin -2.4cm     %I recommend adding these three lines to increase the
\textwidth 16.5cm   %amount of usable space on the page (and save trees)
\textheight 27.5cm

\newcommand{\question}[2] {\vspace{.25in} \hrule\vspace{0.5em}
\noindent{\bf #1: #2} \vspace{0.5em}
\hrule \vspace{.10in}}
\renewcommand{\part}[1] {\vspace{.10in} {\bf (#1)}}

\newcommand{\myname}{Xuan Han}
\newcommand{\myhusky}{han.xua@husky.neu}
\newcommand{\myhwnum}{5}

\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}

\pagestyle{fancyplain}
\lhead{\fancyplain{}{\textbf{HW\myhwnum}}}      % Note the different brackets!
\rhead{\fancyplain{}{\myname\\ \myhusky}}
\chead{\fancyplain{}{8 9 9}}


\begin{document}
\SweaveOpts{concordance=True}

\title{Data Mining Assignment \myhwnum}
\author{\myname \\
        \myhusky}
\date{\today}
\maketitle

\thispagestyle{plain}


\question{8}{Cross validation}
\part{a}
<<8a>>=
set.seed(1)
y = rnorm(100)
x = rnorm(100)
y = x - 2 * x^2 + rnorm(100)
@
{\color{red}
\begin{enumerate}
\item In this data set, n = 100, p = 2
\item $Y$ = $X - 2X^2 + \epsilon$
\end{enumerate}
}


\part{b}
<<8b, fig=TRUE>>=
plot(y ~ x, col = 'blue')
@
{\color{red}
\begin{enumerate}
\item We can see that the data points are distributed along a parabola, just as we expected.
\item x goes from -2 to 2, y goes from -8 to 2.
\end{enumerate}
}

\part{c}
<<8c>>=
library(boot)
set.seed(12)
df = data.frame(X = x, Y = y)
p = 4
loocv.err = rep(NA, p)
for(i in 1:p) {
    glm.fit = glm(Y ~ poly(X, i), data = df)
    loocv.err[i] = cv.glm(df, glm.fit)$delta[1]
}
loocv.err
@


\part{d}
<<8c>>=
set.seed(2015)
loocv.err = rep(NA, p)
for(i in 1:p) {
    glm.fit = glm(Y ~ poly(X, i), data = df)
    loocv.err[i] = cv.glm(df, glm.fit)$delta[1]
}
loocv.err
@
{\color{red}
\begin{enumerate}
\item The results obtained are exactly the same.
\item The reason is that for leave-one-out-CV, the results have nonthing to do with seed. It will evaluate 100 times no matter what the seed is. There is no randomness involved in the process of LOOCV.
\end{enumerate}
}


\part{e}
{\color{red}
\begin{enumerate}
\item The second model have the smallest LSSCV error. This is exactly what I expected.
\item Because the simulated data set is generated by power of 2 of X, if we use a lower order model such as power of 1 of X, it will have high bias; If we use a higher order model such as power of 3 or 4 of X, it will cause high variance. So, power of 2 have low bias and low variance.
\end{enumerate}
}


\part{f}
<<8f>>=
glm.fit = glm(Y ~ poly(X, 1), data = df)
summary(glm.fit)$coeff
glm.fit = glm(Y ~ poly(X, 2), data = df)
summary(glm.fit)$coeff
glm.fit = glm(Y ~ poly(X, 3), data = df)
summary(glm.fit)$coeff
glm.fit = glm(Y ~ poly(X, 4), data = df)
summary(glm.fit)$coeff

@
{\color{red}
\begin{enumerate}
\item From the coefficient estimates we can see that liner and quradradic term of X have vary low p-value while cubic and higher order have high p-value. This results agree with the conclusion dran based on the CV results.
\end{enumerate}
}

\newpage
\question{9}{Surgical}
\part{a}
<<9a>>=
X <- read.table('surgical.txt')
dimnames(X)[[2]] <- c('blood', 'prog', 'enz', 'liver','age',
                      'female', 'modAlc', 'heavyAlc', 'surv', 'lsurv')
X = X[, -9]
N = dim(X)[1]
d = dim(X)[2]
mu.hat = mean(X$lsurv)
mu.hat
@

\part{b}
<<9b>>=
se = sd(X$lsurv) / sqrt(N)
se
@
{\color{red}
\begin{enumerate}
\item This result suggests that the expected deviation of sample mean of lsurv is 0.875
\end{enumerate}
}

\part{c}
<<9c>>=
boot.fn = function(data, index) return (mean(data[index]))
boot(X$lsurv, boot.fn, R = 1000)
@
{\color{red}
\begin{enumerate}
\item We can see that the results are very close. Before se is 0.066896, now it is 0.066885
\end{enumerate}
}

\part{d}
<<8d>>=
confd.low = mu.hat - 2 * se
confd.high = mu.hat + 2 * se
c(confd.low, confd.high)
t.test(X$lsurv)
@
{\color{red}
\begin{enumerate}
\item It is clear that the results are very close.
\end{enumerate}
}

\part{e}
<<9e>>=
md.hat = median(X$lsurv)
md.hat
@

\part{f}
<<9f>>=
boot.fn = function(data, index) return(median(data[index]))
boot(X$lsurv, boot.fn, R = 1000)
@
{\color{red}
\begin{enumerate}
\item Median estimated by bootstrap is 6.046 which is exactly the same above.
\item The std.error estimated by bootstrap is 0.0576, which is small std error.
\end{enumerate}
}

\part{g}
<<9g>>=
quantile(X$lsurv, 0.1)
@

\part{h}
<<9h>>=
boot.fn = function(data, index) return(quantile(data[index], 0.1))
boot(X$lsurv, boot.fn, R = 1000)
@
{\color{red}
\begin{enumerate}
\item The tenth quantile is the same with above.
\item Here we get a small std error for the tenth quantile, which is 0.11
\end{enumerate}
}


\newpage
\question{9}{Ridge and Lasso}
\part{a}
<<99a>>=
set.seed(1)
test = sample(1:N, N / 2)
train = -test
X.train = X[train,]
X.test = X[test, ]
@

\part{b}
<<99b>>=
lm.fit = lm(lsurv ~ ., data = X.train)
mrs = mean((predict.lm(lm.fit, X.test) - X.test$lsurv) ^ 2)
mrs
@

\part{c}
<<99c, fig=True>>=
library(glmnet)
x.train = model.matrix(lsurv ~ ., X.train)[, -1]
y.train = X.train$lsurv
grid = 10 ^ seq(10, -2, length = 100)
ridge.mod = glmnet(x.train, y.train, alpha = 0, lambda = grid, thresh = 1e-12)
cv.out = cv.glmnet(x.train, y.train, alpha = 0, nfolds = 10)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
x.test = model.matrix(lsurv ~ ., X.test)[, -1]
y.test = X.test$lsurv
ridge.pred = predict(ridge.mod, s = bestlam, newx = x.test)
ridge.mrs = mean((ridge.pred - y.test)^2)
ridge.mrs
@

\part{d}
<<99d, fig=True>>=
ridge.mod = glmnet(x.train, y.train, alpha = 1, lambda = grid, thresh = 1e-12)
cv.out = cv.glmnet(x.train, y.train, alpha = 1)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
ridge.pred = predict(ridge.mod, s = bestlam, newx = x.test)
lasso.mrs = mean((ridge.pred - y.test) ^ 2)
lasso.mrs
lasso.coef = predict(cv.out, type = 'coeff', s = bestlam)
lasso.coef
@

\part{g}
{\color{red}
\begin{enumerate}
\item We get a test error of 0.053 for linear model; 0.064 for ridge; 0.054 for lasso
\item The test error get from ridge and lasso are higher than test error directly from a linear mode.
\item From lass we see that coeff of predictor modAlc is set zero.
\end{enumerate}
}

<<99dd>>=
y.test.mu = mean(y.test)
lm.r2 = 1 - mrs / mean((y.test - y.test.mu) ^ 2)
ridge.r2 = 1 - ridge.mrs /mean((y.test - y.test.mu) ^ 2)
lasso.r2 = 1 - lasso.mrs / mean((y.test - y.test.mu) ^ 2)
lm.r2
ridge.r2
lasso.r2
@
{\color{red}
\begin{enumerate}
\item From the $R^2$ statistic we can see that these three model only explained 79\% of the variance of the data, so the accuracy of our model is moderate.
\end{enumerate}
}
\end{document}