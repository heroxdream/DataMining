summary(fit)
par(mfrow = c(2, 2))
plot(fit)
names(fit)
residuals(fit)
names(fit)
effects(fit)
names(fit)
model(fit)
df.residual(fit)
qr(fit)
sample(392, 196)
?sample
?cv.glm
libarary(boot)
library(boot)
?cv.glm
?rnorm
rnorm(10)
library(boot)
set.seed(12)
df = data.frame(X = x, Y = y)
set.seed(1)
y = rnorm(100)
x = rnorm(100)
y = x - 2 * x^2 + rnorm(100)
df = data.frame(X = x, Y = y)
loocv.err = rep(NA, 5)
for(i in 1:5) {
glm.fit = glm(y ~ poly(x, i), df)
loocv.err[i] = cv.glm(df, glm.fit)$delta[1]
}
loocv.err
for(i in 1:5) {
glm.fit = glm(Y ~ poly(X, i), df)
loocv.err[i] = cv.glm(df, glm.fit)$delta[1]
}
View(df)
for(i in 1:5) {
glm.fit = glm(Y ~ poly(X, i), data = df)
loocv.err[i] = cv.glm(df, glm.fit)$delta[1]
}
loocv.err
for(i in 1:p) {
glm.fit = glm(Y ~ poly(X, i), data = df)
summary(glm.fit)
}
p = 4
for(i in 1:p) {
glm.fit = glm(Y ~ poly(X, i), data = df)
summary(glm.fit)
}
for(i in 1:p) {
glm.fit = glm(Y ~ poly(X, i), data = df)
summary(glm.fit)
}
glm.fit = glm(Y ~ poly(X, i), data = df)
summary(glm.fit)
for(i in 1:p) {
glm.fit = glm(Y ~ poly(X, i), data = df)
summary(glm.fit)
}
@
for(i in 1:p) {
glm.fit = glm(Y ~ poly(X, i), data = df)
summary(glm.fit)
}
summ = list()
for(i in 1:p) {
glm.fit = glm(Y ~ poly(X, i), data = df)
summ = c(summ, summary(glm.fit))
}
summ
summ = list()
for(i in 1:p) {
glm.fit = glm(Y ~ poly(X, i), data = df)
summ = c(summ, summary(glm.fit)$cov.scaled)
}
summ
summary(glm.fit)$Coefficients
glm.fit = glm(Y ~ poly(X, 4), data = df)
summary(glm.fit)$Coefficients
glm.fit = glm(Y ~ poly(X, 4), data = df)
summary(glm.fit)$Coefficients
summary(glm.fit)
names(glm.fit)
summary(glm.fit)$coefficients
library(DAAG)
install.packages("library(DAAG)")
install.packages("DAAG")
libaray(DAAG)
libaray(DAAG)
library(DAAG)
?"DAAG"
X <- read.table('surgical')
X <- read.table('surgical.txt')
X <- read.table('./surgical.txt')
X <- read.table('surgical.txt')
setwd("~/Dropbox/neu/tools/github/data_mining/cross_validation")
X <- read.table('surgical.txt')
dimnames(X)[[2]] <- c('blood', 'prog', 'enz', 'liver','age',
'female', 'modAlc', 'heavyAlc', 'surv', 'lsurv')
X
X = X[, -8]
X
X <- read.table('surgical.txt')
dimnames(X)[[2]] <- c('blood', 'prog', 'enz', 'liver','age',
'female', 'modAlc', 'heavyAlc', 'surv', 'lsurv')
X = X[, -9]
X
mean(X$lsurv)
mu.hat = mean(X$lsurv)
mu.hat / sqrt(N)
N = dim(X)[1]
d = dim(X)[2]
mu.hat / sqrt(N)
boot.fn = function(data, index)
return(mean(data[index]) / sqrt(length(data[index])))
boot.fn(X$lsurv, 1:54)
N = dim(X)[1]
d = dim(X)[2]
X
dim(X)
dim(X)[2]
X <- read.table('surgical.txt')
dimnames(X)[[2]] <- c('blood', 'prog', 'enz', 'liver','age',
'female', 'modAlc', 'heavyAlc', 'surv', 'lsurv')
X = X[, -9]
N = dim(X)[1]
d = dim(X)[2]
d
N
boot.fn = function(data, index)
return(mean(data[index]) / sqrt(length(data[index])))
boot.fn(X$lsurv, 1:54)
boot(X$lsurv, boot.fn, R = 1000)
se
se = mu.hat / sqrt(N)
se
t.test(X$lsurv)
confd.low = mu.hat - se
confd.high = mu.hat + se
t.test(X$lsurv)
confd.low
confd.high
se = mu.hat / sqrt(N)
se
boot.fn = function(data, index)
return(mean(data[index]) / sqrt(length(data[index])))
boot(X$lsurv, boot.fn, R = 1000)
se = sd(X$lsurv) / sqrt(N)
se
boot.fn = function(data, index)
return(sd(data[index]) / sqrt(length(data[index])))
boot(X$lsurv, boot.fn, R = 1000)
boot.fn = function(data, index) return (mean(data[index]))
# return(sd(data[index]) / sqrt(length(data[index])))
boot(X$lsurv, boot.fn, R = 1000)
se
confd.low = mu.hat - 2 * se
confd.high = mu.hat + 2 * se
confd.low
confd.high
t.test(X$lsurv)
c(confd.low, confd.high)
t.test(X$lsurv)
meadia(X$blood)
median(X$blood)
median(X$lsurv)
boot.fn = function(data, index) return(median(data[index]))
boot(X$lsurv, boot.fn, R = 1000)
quantile(X$lsurv)
quantile(X$lsurv, 0.1)
boot.fn = function(data, index) return(quantile(data[index], 0.1))
boot(X$lsurv, boot.fn, R = 1000)
test = sample(1:N, N / 2)
train = -test
X.train = X[train,]
X.test = X[test, ]
lm.fit = lm(lsurv ~ ., data = X.train)
?predict
predict.lm()
?predict.lm
predict.lm(lm.fit, X.test)
mean(predict.lm(lm.fit, X.test) - X.test$lsurv)
mean((predict.lm(lm.fit, X.test) - X.test$lsurv)^2)
install.packages("glmnet")
x.train = model.matrix(X.train)
X.train
model.matrix(X.train)
model.matrix(lsurv ~ ., X.train)
y.train = X.train$lsurv
y.train
set.seed(1)
x.train = model.matrix(lsurv ~ ., X.train)
y.train = X.train$lsurv
ridge.mod = glmnet(x.train, y.train, alpha = 0)
cv.out = cv.glmnet(x.train, y.train, alpha = 0)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
x.test = model.matrix(lsurv ~ ., X.test)
y.test = X.test$lsurv
ridge.pred = predict(ridge.mod, s = bestlam, newx = x.test)
mean((ridge.pred - y.test)^2)
set.seed(1)
x.train = model.matrix(lsurv ~ ., X.train)
y.train = X.train$lsurv
ridge.mod = glmnet(x.train, y.train, alpha = 0)
library(glmnet)
ridge.mod = glmnet(x.train, y.train, alpha = 0)
cv.out = cv.glmnet(x.train, y.train, alpha = 0)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
x.test = model.matrix(lsurv ~ ., X.test)
y.test = X.test$lsurv
ridge.pred = predict(ridge.mod, s = bestlam, newx = x.test)
mean((ridge.pred - y.test)^2)
?cv.glmnet
set.seed(1)
x.train = model.matrix(lsurv ~ ., X.train)
y.train = X.train$lsurv
ridge.mod = glmnet(x.train, y.train, alpha = 0)
cv.out = cv.glmnet(x.train, y.train, alpha = 0, nfolds = 27)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
x.test = model.matrix(lsurv ~ ., X.test)
y.test = X.test$lsurv
ridge.pred = predict(ridge.mod, s = bestlam, newx = x.test)
mean((ridge.pred - y.test)^2)
set.seed(1)
x.train = model.matrix(lsurv ~ ., X.train)
y.train = X.train$lsurv
ridge.mod = glmnet(x.train, y.train, alpha = 0)
cv.out = cv.glmnet(x.train, y.train, alpha = 0, nfolds = 5)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
x.test = model.matrix(lsurv ~ ., X.test)
y.test = X.test$lsurv
ridge.pred = predict(ridge.mod, s = bestlam, newx = x.test)
mean((ridge.pred - y.test)^2)
lm.fit = lm(lsurv ~ ., data = X.train)
mean((predict.lm(lm.fit, X.test) - X.test$lsurv) ^ 2)
set.seed(1)
x.train = model.matrix(lsurv ~ ., X.train)
y.train = X.train$lsurv
ridge.mod = glmnet(x.train, y.train, alpha = 0)
cv.out = cv.glmnet(x.train, y.train, alpha = 0)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
x.test = model.matrix(lsurv ~ ., X.test)
y.test = X.test$lsurv
ridge.pred = predict(ridge.mod, s = bestlam, newx = x.test)
mean((ridge.pred - y.test)^2)
cv.out = cv.glmnet(x.train, y.train, alpha = 0, nfolds = 3)
bestlam = cv.out$lambda.min
bestlam
x.test = model.matrix(lsurv ~ ., X.test)
y.test = X.test$lsurv
ridge.pred = predict(ridge.mod, s = bestlam, newx = x.test)
mean((ridge.pred - y.test)^2)
set.seed(1)
x.train = model.matrix(lsurv ~ ., X.train)
y.train = X.train$lsurv
ridge.mod = glmnet(x.train, y.train, alpha = 0)
cv.out = cv.glmnet(x.train, y.train, alpha = 0, nfolds = 10)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
x.test = model.matrix(lsurv ~ ., X.test)
y.test = X.test$lsurv
ridge.pred = predict(ridge.mod, s = bestlam, newx = x.test)
mean((ridge.pred - y.test)^2)
x.train = model.matrix(lsurv ~ ., X.train)
y.train = X.train$lsurv
ridge.mod = glmnet(x.train, y.train, alpha = 0)
cv.out = cv.glmnet(x.train, y.train, alpha = 0, nfolds = 10)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
x.test = model.matrix(lsurv ~ ., X.test)
y.test = X.test$lsurv
ridge.pred = predict(ridge.mod, s = bestlam, newx = x.test)
mean((ridge.pred - y.test)^2)
ridge.mod = glmnet(x.train, y.train, alpha = 1)
cv.out = cv.glmnet(x.train, y.train, alpha = 1)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
ridge.pred = predict(ridge.mod, s = bestlam, newx = x.test)
mean((ridge.pred - y.test)^2)
test = sample(1:N, N / 3)
train = -test
X.train = X[train,]
X.test = X[test, ]
lm.fit = lm(lsurv ~ ., data = X.train)
mean((predict.lm(lm.fit, X.test) - X.test$lsurv) ^ 2)
x.train = model.matrix(lsurv ~ ., X.train)
y.train = X.train$lsurv
ridge.mod = glmnet(x.train, y.train, alpha = 0)
cv.out = cv.glmnet(x.train, y.train, alpha = 0, nfolds = 10)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
x.test = model.matrix(lsurv ~ ., X.test)
y.test = X.test$lsurv
ridge.pred = predict(ridge.mod, s = bestlam, newx = x.test)
mean((ridge.pred - y.test)^2)
ridge.mod = glmnet(x.train, y.train, alpha = 1)
cv.out = cv.glmnet(x.train, y.train, alpha = 1)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
ridge.pred = predict(ridge.mod, s = bestlam, newx = x.test)
mean((ridge.pred - y.test)^2)
test = sample(1:N, N / 4)
train = -test
X.train = X[train,]
X.test = X[test, ]
lm.fit = lm(lsurv ~ ., data = X.train)
mean((predict.lm(lm.fit, X.test) - X.test$lsurv) ^ 2)
x.train = model.matrix(lsurv ~ ., X.train)
y.train = X.train$lsurv
ridge.mod = glmnet(x.train, y.train, alpha = 0)
cv.out = cv.glmnet(x.train, y.train, alpha = 0, nfolds = 10)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
x.test = model.matrix(lsurv ~ ., X.test)
y.test = X.test$lsurv
ridge.pred = predict(ridge.mod, s = bestlam, newx = x.test)
mean((ridge.pred - y.test)^2)
ridge.mod = glmnet(x.train, y.train, alpha = 1)
cv.out = cv.glmnet(x.train, y.train, alpha = 1)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
ridge.pred = predict(ridge.mod, s = bestlam, newx = x.test)
mean((ridge.pred - y.test)^2)
test = sample(1:N, N / 2)
train = -test
X.train = X[train,]
X.test = X[test, ]
@
\part{b}
<<99b>>=
lm.fit = lm(lsurv ~ ., data = X.train)
mean((predict.lm(lm.fit, X.test) - X.test$lsurv) ^ 2)
@
\part{c}
<<99c, fig=True>>=
library(glmnet)
x.train = model.matrix(lsurv ~ ., X.train)
y.train = X.train$lsurv
ridge.mod = glmnet(x.train, y.train, alpha = 0)
cv.out = cv.glmnet(x.train, y.train, alpha = 0, nfolds = 10)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
x.test = model.matrix(lsurv ~ ., X.test)
y.test = X.test$lsurv
ridge.pred = predict(ridge.mod, s = bestlam, newx = x.test)
mean((ridge.pred - y.test)^2)
@
\part{d}
<<99d, fig=True>>=
ridge.mod = glmnet(x.train, y.train, alpha = 1)
cv.out = cv.glmnet(x.train, y.train, alpha = 1)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
ridge.pred = predict(ridge.mod, s = bestlam, newx = x.test)
mean((ridge.pred - y.test)^2)
test = sample(1:N, N / 2)
train = -test
X.train = X[train,]
X.test = X[test, ]
lm.fit = lm(lsurv ~ ., data = X.train)
mean((predict.lm(lm.fit, X.test) - X.test$lsurv) ^ 2)
x.train = model.matrix(lsurv ~ ., X.train)
y.train = X.train$lsurv
ridge.mod = glmnet(x.train, y.train, alpha = 0)
cv.out = cv.glmnet(x.train, y.train, alpha = 0, nfolds = 10)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
x.test = model.matrix(lsurv ~ ., X.test)
y.test = X.test$lsurv
ridge.pred = predict(ridge.mod, s = bestlam, newx = x.test)
mean((ridge.pred - y.test)^2)
ridge.mod = glmnet(x.train, y.train, alpha = 1)
cv.out = cv.glmnet(x.train, y.train, alpha = 1)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
ridge.pred = predict(ridge.mod, s = bestlam, newx = x.test)
mean((ridge.pred - y.test)^2)
set.seed(1)
test = sample(1:N, N / 2)
train = -test
X.train = X[train,]
X.test = X[test, ]
lm.fit = lm(lsurv ~ ., data = X.train)
mean((predict.lm(lm.fit, X.test) - X.test$lsurv) ^ 2)
x.train = model.matrix(lsurv ~ ., X.train)
y.train = X.train$lsurv
ridge.mod = glmnet(x.train, y.train, alpha = 0)
cv.out = cv.glmnet(x.train, y.train, alpha = 0, nfolds = 10)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
x.test = model.matrix(lsurv ~ ., X.test)
y.test = X.test$lsurv
ridge.pred = predict(ridge.mod, s = bestlam, newx = x.test)
mean((ridge.pred - y.test)^2)
ridge.mod = glmnet(x.train, y.train, alpha = 1)
cv.out = cv.glmnet(x.train, y.train, alpha = 1)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
ridge.pred = predict(ridge.mod, s = bestlam, newx = x.test)
mean((ridge.pred - y.test)^2)
ridge.mod = glmnet(x.train, y.train, alpha = 1)
ridge.mod
10^ seq(10, -2, lenght = 100)
10^ seq(10, -2, length = 100)
library(glmnet)
x.train = model.matrix(lsurv ~ ., X.train)
y.train = X.train$lsurv
grid = 10 ^ seq(10, -2, length = 100)
ridge.mod = glmnet(x.train, y.train, alpha = 0, lambda = grid, thresh = 1e-12)
cv.out = cv.glmnet(x.train, y.train, alpha = 0, nfolds = 10)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
x.test = model.matrix(lsurv ~ ., X.test)
y.test = X.test$lsurv
ridge.pred = predict(ridge.mod, s = bestlam, newx = x.test)
mean((ridge.pred - y.test)^2)
lasso.coef = predict(cv.out, type = 'coeff', s = bestlam)
lasso.coef
ridge.mod = glmnet(x.train, y.train, alpha = 1, lambda = grid, thresh = 1e-12)
cv.out = cv.glmnet(x.train, y.train, alpha = 1)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
lasso.coef = predict(cv.out, type = 'coeff', s = bestlam)
lasso.coef
ridge.pred = predict(ridge.mod, s = bestlam, newx = x.test)
mean((ridge.pred - y.test)^2)
@
lasso.coef
lasso.coef[1:20, ]
lasso.coef[1:10, ]
x.train = model.matrix(lsurv ~ ., X.train)[, -1]
x.train
X.train
x.train[, -2]
X.train
head(X.train)
head(X.train[, -1])
head(X.train[, -2])
x.train = model.matrix(lsurv ~ ., data = X.train)
head(x.train)
head(x.train)[,-1]
y.test.mu = mean(y.test)
y.test.mu = mean(y.test)
ridge.r2 = 1 - ridge.mrs /mean((y.test - y.test.mu)^2)
mrs = mean((ridge.pred - y.test)^2)
ridge.mrs
ridge.mrs = mean((ridge.pred - y.test)^2)
ridge.mrs
lasso.mrs = mean((ridge.pred - y.test) ^ 2)
lasso.mrs
library(glmnet)
x.train = model.matrix(lsurv ~ ., X.train)[, -1]
y.train = X.train$lsurv
grid = 10 ^ seq(10, -2, length = 100)
ridge.mod = glmnet(x.train, y.train, alpha = 0, lambda = grid, thresh = 1e-12)
cv.out = cv.glmnet(x.train, y.train, alpha = 0, nfolds = 10)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
x.test = model.matrix(lsurv ~ ., X.test)[, -1]
y.test = X.test$lsurv
ridge.pred = predict(ridge.mod, s = bestlam, newx = x.test)
ridge.mrs = mean((ridge.pred - y.test)^2)
ridge.mrs
ridge.mod = glmnet(x.train, y.train, alpha = 1, lambda = grid, thresh = 1e-12)
cv.out = cv.glmnet(x.train, y.train, alpha = 1)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
ridge.pred = predict(ridge.mod, s = bestlam, newx = x.test)
lasso.mrs = mean((ridge.pred - y.test) ^ 2)
lasso.mrs
lasso.coef = predict(cv.out, type = 'coeff', s = bestlam)
lasso.coef
y.test.mu = mean(y.test)
ridge.r2 = 1 - ridge.mrs /mean((y.test - y.test.mu)^2)
ridge.r2
lasso.r2 = 1 - lasso.mrs / mean((y.test - y.test.mu) ^ 2)
lasso.r2
lm.fit = lm(lsurv ~ ., data = X.train)
mrs = mean((predict.lm(lm.fit, X.test) - X.test$lsurv) ^ 2)
mrs
lm.r2 = 1 - mrs / mean((y.test - y.test.mu) ^ 2)
lm.r2
