\documentclass[a4paper]{article}

\usepackage{Sweave} %--------------------------------!
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage[usenames, dvipsnames]{color}
\usepackage{verbatim}

\oddsidemargin0cm
\topmargin-2cm     %I recommend adding these three lines to increase the
\textwidth16.5cm   %amount of usable space on the page (and save trees)
\textheight23.5cm

\newcommand{\question}[2] {\vspace{.25in} \hrule\vspace{0.5em}
\noindent{\bf #1: #2} \vspace{0.5em}
\hrule \vspace{.10in}}
\renewcommand{\part}[1] {\vspace{.10in} {\bf (#1)}}

\newcommand{\myname}{Xuan Han}
\newcommand{\myhusky}{han.xua@husky.neu}
\newcommand{\myhwnum}{2}

\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}

\pagestyle{fancyplain}
\lhead{\fancyplain{}{\textbf{HW\myhwnum}}}      % Note the different brackets!
\rhead{\fancyplain{}{\myname\\ \myhusky}}
\chead{\fancyplain{}{10 8 11}}


\begin{document}
\SweaveOpts{concordance=True}

\title{Data Mining Assignment \myhwnum}
\author{\myname \\
        \myhusky}
\date{\today}
\maketitle

\medskip
\thispagestyle{plain}

\question{10}{Boston data set explore}
\part{a}
First, have a look at the dataset.
<<boston>>=
library(MASS)
summary(Boston)
str(Boston)
attach(Boston)
@

\begin{enumerate}
{
\color{red}
\item The Boston data frame has 506 rows and 14 columns.
\item All of the colnums are numerical values, which contains quantitive information like crime rate, nitrogen oxides concentration, average room number, age, tax.
\item However, there is one colnum feature, chas, is actually a binary YES/NO category value which is represented by 1/0.
}
\end{enumerate}

\part{b}
<<pairboston, fig=TRUE>>=
par(mfrow = c(3,2))
boxplot(crim ~ chas, main = "chas vs. crime", col = rainbow(7), xlab = "chas", ylab = "crime")
plot(y = medv, x = rm, main = "medv vs. rm", col = rainbow(7), ylab = "medv", xlab = "rm")
plot(y = nox, x = dis, main = "nox vs. dis", col = rainbow(7), ylab = "nox", xlab = "dis")
plot(y = nox, x = age, main = "nox vs. age", col = rainbow(7), ylab = "nox", xlab = "age")
plot(y = crim, x = rad, main = "crim vs. rad", col = rainbow(7), ylab = "crim", xlab = "rad")
plot(y = rm, x = tax, main = "rm vs. tax", col = rainbow(7), ylab = "rm", xlab = "tax")
@
\begin{enumerate}
{
\color{red}
\item we can see the crime rate in Boston is low in overall. However, places not bounded with Charles River are more likely have higher crime rate.
\item The more room number, the higher the median value of the house.
\item The futher the distance from downtown, the lower nitrogen oxides concentration
\item The more propotion of old house, the higher nitrogen oxides concentration
\item It seems within 10 unit accessibility to radial highways the crime is low, it goes ridicaly when the index pass 20.
\item It seems there is no obvious relationship between tax and room number.
}
\end{enumerate}

\part{c}
Yes, there are. Just as I have described in (b), Charles River and rad are good predictors. Besides, I find that distance to Boston employment centres is also a good predictor.
<<crime, fig=TRUE>>=
plot(y = crim, x = dis, main = "crim vs. dis", col = rainbow(7), ylab = "crim", xlab = "dis")
@
{\color{red}\\
I find that when the distance is around 2 unit, the crime rate is very hight. With distance goes on, crime rate keep at the same level.
}

\part{d}
<<high, fig=TRUE>>=
par(mfrow = c(1, 3))
boxplot(crim, col = rainbow(7), main = "crime rate")
boxplot(tax, col = rainbow(7), main = "tax rate")
boxplot(ptratio, col = rainbow(7), main = "pupil-teacher ratio")
@
<<hist, fig=TRUE>>=
par(mfrow = c(1, 3))
hist(crim)
hist(tax)
hist(ptratio)
summary(crim)
summary(tax)
summary(ptratio)
@
\begin{enumerate}
{\color{red}
\item We can see that some subursbs of Boston really have particularly high crime rate and tax rate. But not pupil-teacher ratios. Instead, there are particularly low pupil-teacher ratios.
\item For both crime rate and tax rate, there are instances that are far away from there median value.
\item The range of crime is between 0.00632 and 88.9, the gap is quite huge. This means there are very good areas and extremly bad areas. Fortunately, for most of the areas the crime rate is below 10.
\item The range of tax is between 187 and 711. But we dont have data about tax between 500 to 600. There are also very high tax rate that away from the median of the data set.
\item The range of pupil-teacher ratio is between 12.6 and 22, which is not a huge gap. This means the resources of education is relatively fare.
}
\end{enumerate}


\part{e}
<<chas>>=
sum(Boston$chas)
@
{\color{red}
35 suburbs are bounded to Charles River.}


\part{f}
<<>>=
summary(ptratio)
@
{\color{red}
The median pupil-teacher ratio among the towns in this data set is 19.05}


\part{g}
<<>>=
Boston[Boston$medv == summary(medv)[1],]
round(sapply(Boston, mean), 2)
@

\begin{enumerate}
{\color{red}
\item subsurb 399 and 406 have the lowest medv, which is 5
\item Compared with other areas, these two areas have very high crim rate, much higher zn index, with more non-retail business. Those houses in the two areas are very old. They are far away from radial highways, with lower lstat.
}
\end{enumerate}


\part{h}
<<dewlling, fig=TRUE>>=
sum(Boston$rm > 7)
sum(Boston$rm > 8)
boxplot(Boston[Boston$rm > 8,])
round(sapply(Boston[Boston$rm > 8,], mean), 2)
round(sapply(Boston, mean), 2)
@

\begin{enumerate}
{\color{red}
\item 64 suburbs average more than seven rooms per dwelling
\item 13 suburbs average more than eight rooms per dwelling
\item Compared with other areas, these areas have much less crime rate, lower lstat index, but much higher medv. These areas have less people and much safer.
}
\end{enumerate}



\question{8}{lm on Auto}
\part{a}
<<a>>=
library(ISLR)
attach(Auto)
fit = lm(mpg ~ horsepower, data = Auto)
summary(fit)
new.data = data.frame(horsepower = 98, mpg = 0)
predict.lm(fit, new.data)
confint(fit, level = 0.9)
predict.lm(fit, new.data, interval = c('pred'))
predict.lm(fit, new.data, interval = c('con'))
@
\begin{enumerate}
{\color{red}
\item There for sure is a negative relationship between the reponse and predictor. The more horsepower, the less mpg.
\item At the begining, mpg decrease very quickly with horsepower increase. In the end, mpg does not change even horsepower increase.
\item pedicted value is 24.467, 95\% confidence interval is [23.97, 24.96] while predict interval is [14.81, 34,12]
}
\end{enumerate}


\part{b}
<<b, fig=TRUE>>=
plot(mpg ~ horsepower, data = Auto, col = "blue")
abline(fit)
@
{\color{red}\\ See above figure.}


\part{c}
<<c, fig=TRUE>>=
plot(residuals(fit))
abline(h = 0)
@

{
\color{red}
From the residual plot we can see that the linear is not a good fit. The mean of the residual is not zero. For the left part, most of the resudial points are below the line while for the right part they are above the line.
}


\question{11}{t-statistic}
<<pre>>=
set.seed(1)
x = rnorm(100)
y = 2 * x + rnorm(100)
@

\part{a}
<<a, fig=TRUE>>=
fit = lm(y ~ x + 0)
summary(fit)
plot(x, y)
abline(fit)
@
\begin{enumerate}
{\color{red}
\item From the summary of the model, we can see that:
\begin{itemize}
\item estimated $\hat{\beta}$ is 1.9939
\item standard error of this coefficient estimate is 0.1065, small relatatively to $\hat{\beta}$
\item t-statistic is 18.73
\item p-value is $2.2 * 10^{-16}$
\end{itemize}
This result suppose that the null hypothesis: $H_0$ : $\beta = 0$ is rejected. There must be an association between x and y.
}
\end{enumerate}


\part{b}
<<b, fig=TRUE>>=
fit = lm(x ~ y + 0)
summary(fit)
plot(y, x)
abline(fit)
@

\begin{enumerate}
{\color{red}
\item From the summary of the model, we can see that:
\begin{itemize}
\item estimated $\hat{\beta}$ is 0.39111
\item standard error of this coefficient estimate is 0.02089, small relatively to $\hat{\beta}$
\item t-statistic is 18.73
\item p-value is $2.2 * 10^{-16}$
\end{itemize}
Again, this result suppose that the null hypothesis: $H_0$ : $\beta = 0$ is rejected. There must be an association between x and y.
}
\end{enumerate}

\part{c}
\begin{enumerate}
{\color{red}
\item They have exactly the same t-value, r-squared, F-value and p-value.
\item They both rejected the null hypothesis
}
\end{enumerate}


\part{d}
\begin{proof}
Since x is generated with zero mean $\implies \bar{x} = \bar{y} = 0$\\
$\implies \beta = \frac{\sum_{i}^{n} x_iy_i}{\sum_{i}^{n}x_i^2}$\\
Thus:
\begin{eqnarray*}
t &=&\frac{\hat{\beta}}{SE(\hat{\beta})}\\
  &=&\frac{\sum_{i}^{n} x_iy_i}{\sum_{i}^{n}x_i^2} \sqrt{\frac{(n-1)\sum_i^n x_i^2}{\sum_i^n (y_i-x_i\hat{\beta})}}\\
  &=&\frac{\sqrt{n-1}\sum_i^n x_iy_i}{\sqrt{\sum_i^n x_i^2 \sum_i^n (y_i - x_i \hat{\beta})^2}}\\
  &=&\frac{\sqrt{n-1}\sum_i^n x_iy_i}{\sqrt{\sum_i^n x_i^2 \sum_i^n (y_i^2 + (x_i\hat{\beta})^2 - 2y_ix_i\hat{\beta})}}\\
  &=&\frac{\sqrt{n-1}\sum_i^n x_iy_i}{\sqrt{\sum_i^nx_i^2\sum_i^ny_i^2 - \sum_i^nx_i^2\hat{\beta}(2\sum_i^nx_iy_i - \hat{\beta}\sum_i^nx_i^2)}}\\
  &&now, plug in \hat{\beta}\\
  &=& \frac{\sqrt{n-1}\sum_i^n x_iy_i}{\sqrt{\sum_i^nx_i^2\sum_i^ny_i^2 - \sum_i^nx_iy_i(2\sum_i^nx_iy_i - \sum_i^nx_iy_i)}}\\
  &=& \frac{\sqrt{n-1}\sum_i^n x_iy_i}{\sqrt{\sum_i^nx_i^2\sum_i^ny_i^2 - (\sum_i^nx_iy_i)^2}}\\
\end{eqnarray*}
\end{proof}
<<confirt>>=
(sqrt(length(x) - 1) * sum(x*y)) / (sqrt(sum(x*x) * sum(y*y) - (sum(x*y)) ^ 2))
@

\part{e}
{\color{red}
This is pretty simple: the equation we inferred above is symmetric for both x and y. So change the postion of x and does not change t-value}

\part{f}
<<f>>=
fit = lm(y ~ x)
summary(fit)

fit = lm(x ~ y)
summary(fit)
@

{\color{red}
As shown above, the t-value for $\hat{\beta^1}$ are both 18.56

}

\end{document}