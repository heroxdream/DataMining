\documentclass[a4paper]{article}

\usepackage{Sweave} %--------------------------------!
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage[usenames, dvipsnames]{color}
\usepackage{verbatim}

\oddsidemargin0cm
\topmargin-2cm     %I recommend adding these three lines to increase the
\textwidth16.5cm   %amount of usable space on the page (and save trees)
\textheight23.5cm

\newcommand{\question}[2] {\vspace{.25in} \hrule\vspace{0.5em}
\noindent{\bf #1: #2} \vspace{0.5em}
\hrule \vspace{.10in}}
\renewcommand{\part}[1] {\vspace{.10in} {\bf (#1)}}

\newcommand{\myname}{Xuan Han}
\newcommand{\myhusky}{han.xua@husky.neu}
\newcommand{\myhwnum}{4}

\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}

\pagestyle{fancyplain}
\lhead{\fancyplain{}{\textbf{HW\myhwnum}}}      % Note the different brackets!
\rhead{\fancyplain{}{\myname\\ \myhusky}}
\chead{\fancyplain{}{8 11}}


\begin{document}
\SweaveOpts{concordance=True}

\title{Data Mining Assignment \myhwnum}
\author{\myname \\
        \myhusky}
\date{\today}
\maketitle

\thispagestyle{plain}

\question{8}{Best subset selection \& forward \& backwards selection}
\part{a}
<<8a>>=
library(leaps)
set.seed(2015)
X = rnorm(100)
es = rnorm(100)
@

\part{b}
<<8b>>=
beta = c(2, 3, 4, 10)
Y = beta[1] + beta[2] * X + beta[3] * X ^ 2 + beta[4] * X ^ 3 + es
# plot(Y ~ X)
@

\part{c}
<<8c>>=
df = data.frame(y = Y, x = X)
fit.full = regsubsets(y ~ poly(x, 10, raw = T), df, nvmax = 10)
fit.summary = summary(fit.full)
fit.summary
which.max(fit.summary$adjr2)
which.min(fit.summary$bic)
@

<<8cfig, fig=True>>=
par(mfrow = c(2, 1))
plot(fit.summary$adjr2, xlab = 'Number of varibles', ylab = 'Ajusted R-squre', type = 'l')
points(9, fit.summary$adjr2[3], col = 'red', cex = 2, pch = 20)
plot(fit.summary$bic, xlab = 'Number of varibles', ylab = 'BIC', type = 'l')
points(3, fit.summary$bic[3], col = 'red', cex = 2, pch = 20)
coef(fit.full, 3)
coef(fit.full, 9)
@
{\color{red}
\begin{enumerate}
\item Result of this problem is very UNSTABLE. Many facts will influence the final result of this problem:
\begin{itemize}
\item seed
\item if we set raw = T in ploy function
\item most importantly, how we choose beta.
\end{itemize}
Sometime, I get max adjr2 at index 7, sometimes at index 9 or 5.\\
But fortunately, BIC is relatively stable than adjr2, most of the time I get minimum BIC at index 3. Because BIC is more favorable to smaller number of variables.\\
\item As shown in the figure above, adjr2 reached its peak at index 3, and keep stable so on.
BIC reached its global minimum value at index 3, and this is the only minimum value.
\item The best predicted coef by BIC are close to its true value, as shown above.
\item However, the best predicted coef by adjr2 is not so, as shown above.
\end{enumerate}
}

\part{d}
<<8df, fig=TRUE>>=
fit.fwd = regsubsets(y ~ poly(x, 10, raw = T), df, nvmax = 10, method = 'forward')
fit.fwd.summary = summary(fit.fwd)
# fit.fwd.summary
which.max(fit.fwd.summary$adjr2)
which.min(fit.fwd.summary$bic)
par(mfrow = c(2, 1))
plot(fit.fwd.summary$adjr2, xlab = 'Number of varibles', ylab = 'Ajusted R-squre', type = 'l')
points(10, fit.fwd.summary$adjr2[3], col = 'red', cex = 2, pch = 20)
plot(fit.fwd.summary$bic, xlab = 'Number of varibles', ylab = 'BIC', type = 'l')
points(3, fit.fwd.summary$bic[3], col = 'red', cex = 2, pch = 20)
coef(fit.fwd, 3)
coef(fit.fwd, 10)
@
{\color{red}
\begin{enumerate}
\item Forward selection, basically, does not change too much from full selection.
\item This time adjr2 choose model at index 10. And BIC still choose model at index 3.
\item The coef of model at index 3 is still close to the true value.
\end{enumerate}
}
<<8db, fig=TRUE>>=
fit.bwd = regsubsets(y ~ poly(x, 10, raw = T), df, nvmax = 10, method = 'backward')
fit.bwd.summary = summary(fit.bwd)
# fit.bwd.summary
which.max(fit.bwd.summary$adjr2)
which.min(fit.bwd.summary$bic)
par(mfrow = c(2, 1))
plot(fit.bwd.summary$adjr2, xlab = 'Number of varibles', ylab = 'Ajusted R-squre', type = 'l')
points(9, fit.bwd.summary$adjr2[3], col = 'red', cex = 2, pch = 20)
plot(fit.bwd.summary$bic, xlab = 'Number of varibles', ylab = 'BIC', type = 'l')
points(3, fit.bwd.summary$bic[3], col = 'red', cex = 2, pch = 20)
coef(fit.bwd, 3)
coef(fit.fwd, 9)
@
{\color{red}
\begin{enumerate}
\item Backward selection, also, does not change too much from full selection.
\item This time adjr2 choose model at index 9. And BIC still choose model at index 3.
\item The coef of model at index 3 is still close to the true value.
\end{enumerate}
}

\newpage

\question{11 - 1}{Predict per capita crime rate in Boston}
{\color{red}
\begin{enumerate}
\item Since Boston dataset only has 506 data points, we should use 10-folds cross validation to justify performance of model.\\
\item We use best selection to select candidate models, since there are only 13 predictors.\\
\end{enumerate}
}
\part{a \& b} best selection \& cross-validation\\
<<111a, fig=T>>=
library(MASS)
bos = Boston
attach(bos)

predict.regsubsets = function(object, newdata, id, ...) {
    form = as.formula(object$call[[2]])
    mat = model.matrix(form, newdata)
    coefi = coef(object, id = id)
    xvars = names(coefi)
    mat[, xvars] %*% coefi
}

set.seed(0)
k = 10
pred.count = ncol(bos) - 1
folds = sample(1:k, nrow(bos), replace = T)
err.matrix = matrix(NA, k, pred.count)
for (i in 1:k) {
    fit.best = regsubsets(crim ~ ., bos[folds != i, ], nvmax = 13)
    for (j in 1:pred.count) {
#         if (j == 9) {
#             print(coef(fit.best, j))
#         }
        pred = predict(fit.best, bos[folds == i, ], id = j)
        err.matrix[i, j] = mean((bos$crim[folds == i] - pred) ^ 2)
    }
}
mean.err = apply(err.matrix, 2, mean)
which.min(mean.err)
mean.err[which.min(mean.err)]
plot(mean.err, pch = 20, type = "b", col = 'red')
fit.best = regsubsets(crim ~ ., bos, nvmax = 13)
coef(fit.best, which.min(mean.err))
@

{\color{red}
\begin{enumerate}
\item Finally, it's easy to see that with 9 predictors we get the best perfromance on CV.
\item The best predictors and its coefficients are shown above.
\end{enumerate}
}

\part{c} analysis
{\color{red}
\begin{enumerate}
\item The finaly choosen model does not contain all of the features in the data sets.
\item Because as shown in the experiment above, model with 9 predictors has the lowest MSE on cross-validation sets.
\end{enumerate}
}

\newpage
\question{11 - 2}{Predict lsurv in Surgical}
{\color{red}
\begin{enumerate}
\item Since Surgical dataset only has 54 data points, I choose to use 5-folds cross validation to justify performance of model.\\
\item We use best selection to select candidate models, since there are only 8 predictors.\\
\item We only use log-survival time as reponse.
\end{enumerate}
}
\part{a \& b} best selection \& cross-validation\\
<<112a, fig=T>>=
rawdata = read.table('surgical.txt')
surgical = rawdata[, -9]
dimnames(surgical)[[2]] <- c('blood', 'prog', 'enz', 'liver',
                      'age', 'female', 'modAlc', 'heavyAlc', 'lsurv')
set.seed(0)
k = 5
pred.count = ncol(surgical) - 1
folds = sample(1:k, nrow(surgical), replace = T)
err.matrix = matrix(NA, k, pred.count)
for (i in 1:k) {
    fit.best = regsubsets(lsurv ~ ., surgical[folds != i, ], nvmax = 8)

    for (j in 1:pred.count) {
#         if (j == 4) {
#             print(coef(fit.best, j))
#         }
        pred = predict(fit.best, surgical[folds == i, ], id = j)
        err.matrix[i, j] = mean((surgical$lsurv[folds == i] - pred) ^ 2)
    }
}
mean.err = apply(err.matrix, 2, mean)
which.min(mean.err)
mean.err[which.min(mean.err)]
plot(mean.err, pch = 20, type = "b", col = 'red')
fit.best = regsubsets(lsurv ~ ., surgical, nvmax = 8)
coef(fit.best, which.min(mean.err))
@
{\color{red}
\begin{enumerate}
\item Finally, it's easy to see that with 4 predictors we get the best perfromance on CV.
\item The best predictors and its coefficients are shown above.
\end{enumerate}
}

\part{c}
{\color{red}
\begin{enumerate}
\item Obviously my best model does not contain all the predictors. Since with 4 predictors my model get lowest MSE on CV.
\end{enumerate}
}

\end{document}